{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report 2\n",
    "\n",
    "Natasha Kamtekar \n",
    "nak142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Formatting\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First i'm going to unpickle all the necessary data I need for this notebook. I went ahead and pickled the data I worked on in [progress report 1](https://github.com/Data-Science-for-Linguists-2020/Twitter-Positivity-Analysis/blob/master/notebooks/data_parsing.ipynb) as well as the datasets for the [classifier]() I generated. \n",
    "\n",
    "I'm flashing the 2011/2019 tweets, but setting the classifier data aside until later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tweets2011.pkl', 'rb')      \n",
    "tweets2011 = pickle.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tweets2019.pkl', 'rb')      \n",
    "tweets2019 = pickle.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('classifier.pkl', 'rb')      \n",
    "classifier = pickle.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word_features.pkl', 'rb')      \n",
    "word_features = pickle.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user.lang</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RT @yukichii927: Ëµ∑„Åç„ÅüÔºü„Åä„ÅØ„Çà„ÅÜ‚ú®\\n\\nÊò®Êó•ÊÄù„ÅÑ„Åå„Åë„Åö„ÄÅÊ≤¢Â±±„ÅÆ„Ç≥„É°„É≥„Éà„Çí...</td>\n",
       "      <td>ja</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(ŸàŸÑŸÉŸÑ ÿ£ŸÖÿ© ÿ£ÿ¨ŸÑ ŸÅÿ•ÿ∞ÿß ÿ¨ÿßÿ° ÿ£ÿ¨ŸÑŸáŸÖ ŸÑÿß Ÿäÿ≥ÿ™ÿ£ÿÆÿ±ŸàŸÜ ÿ≥ÿßÿπÿ© ...</td>\n",
       "      <td>ar</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RT @elconfidencial: La chapuza de la operaci√≥n...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RT @bpswag27: ‡πÑ‡∏ñ‡∏ó‡∏ß‡∏¥‡∏ï‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏î‡∏° ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á...</td>\n",
       "      <td>th</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@onion__017 Ê±ö„Å≠„ÅáÂÖÉÊ∞óÁéâüòÇwww</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6462</td>\n",
       "      <td>ÿ≠ÿ≥ÿ®Ÿä ÿßŸÑŸÑŸá ŸÑÿß ÿ•ŸÑŸá ÿ•ŸÑÿß ŸáŸà ÿπŸÑŸäŸá ÿ™ŸàŸÉŸÑÿ™ ŸàŸáŸà ÿ±ÿ® ÿßŸÑÿπÿ±...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed May 01 06:31:45 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6463</td>\n",
       "      <td>@llixa_16 ÿßŸàÿßŸáŸáŸá ŸäŸÉÿ´ÿ±ŸáŸÖ ÿπŸÜÿØ ÿ±ÿ®Ÿä</td>\n",
       "      <td>ar</td>\n",
       "      <td>Wed May 01 06:31:45 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6464</td>\n",
       "      <td>‡πÅ‡∏≠‡∏Ñ‡∏ó‡∏µ‡∏ü‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡πÅ‡∏≠‡∏û ‡∏ó‡∏ß‡∏¥‡∏ó ‡πÑ‡∏≠‡∏à‡∏µ ‡∏¢‡∏π‡∏ó‡∏π‡∏õ ‡∏à‡∏ö</td>\n",
       "      <td>th</td>\n",
       "      <td>Wed May 01 06:31:45 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6465</td>\n",
       "      <td>Ewan</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed May 01 06:31:45 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6466</td>\n",
       "      <td>Â≠ê‰æõ„Åü„Å°„ÅØ„Çπ„Éù„É≥„Ç∏„Éú„Éñ„Åå„Åô„Åî„ÅèÂ•Ω„Åç„ÅßÂ§¢‰∏≠„ÅßË¶≥„Çã„Çì„Å†„Åë„Å©„ÄÅÁßÅ„ÅØË¶≥„Å¶„Çã„Å®‰∏çÂÆâ„Å´„Å™„Çã„Çì„Å†‚Ä¶„ÅÇ„ÇåÊÄñ„Åè...</td>\n",
       "      <td>ja</td>\n",
       "      <td>Wed May 01 06:31:45 +0000 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6467 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text user.lang  \\\n",
       "0     RT @yukichii927: Ëµ∑„Åç„ÅüÔºü„Åä„ÅØ„Çà„ÅÜ‚ú®\\n\\nÊò®Êó•ÊÄù„ÅÑ„Åå„Åë„Åö„ÄÅÊ≤¢Â±±„ÅÆ„Ç≥„É°„É≥„Éà„Çí...        ja   \n",
       "1     (ŸàŸÑŸÉŸÑ ÿ£ŸÖÿ© ÿ£ÿ¨ŸÑ ŸÅÿ•ÿ∞ÿß ÿ¨ÿßÿ° ÿ£ÿ¨ŸÑŸáŸÖ ŸÑÿß Ÿäÿ≥ÿ™ÿ£ÿÆÿ±ŸàŸÜ ÿ≥ÿßÿπÿ© ...        ar   \n",
       "2     RT @elconfidencial: La chapuza de la operaci√≥n...        en   \n",
       "3     RT @bpswag27: ‡πÑ‡∏ñ‡∏ó‡∏ß‡∏¥‡∏ï‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏î‡∏° ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á...        th   \n",
       "4                                @onion__017 Ê±ö„Å≠„ÅáÂÖÉÊ∞óÁéâüòÇwww        en   \n",
       "...                                                 ...       ...   \n",
       "6462  ÿ≠ÿ≥ÿ®Ÿä ÿßŸÑŸÑŸá ŸÑÿß ÿ•ŸÑŸá ÿ•ŸÑÿß ŸáŸà ÿπŸÑŸäŸá ÿ™ŸàŸÉŸÑÿ™ ŸàŸáŸà ÿ±ÿ® ÿßŸÑÿπÿ±...        en   \n",
       "6463                    @llixa_16 ÿßŸàÿßŸáŸáŸá ŸäŸÉÿ´ÿ±ŸáŸÖ ÿπŸÜÿØ ÿ±ÿ®Ÿä        ar   \n",
       "6464             ‡πÅ‡∏≠‡∏Ñ‡∏ó‡∏µ‡∏ü‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡πÅ‡∏≠‡∏û ‡∏ó‡∏ß‡∏¥‡∏ó ‡πÑ‡∏≠‡∏à‡∏µ ‡∏¢‡∏π‡∏ó‡∏π‡∏õ ‡∏à‡∏ö        th   \n",
       "6465                                               Ewan        en   \n",
       "6466  Â≠ê‰æõ„Åü„Å°„ÅØ„Çπ„Éù„É≥„Ç∏„Éú„Éñ„Åå„Åô„Åî„ÅèÂ•Ω„Åç„ÅßÂ§¢‰∏≠„ÅßË¶≥„Çã„Çì„Å†„Åë„Å©„ÄÅÁßÅ„ÅØË¶≥„Å¶„Çã„Å®‰∏çÂÆâ„Å´„Å™„Çã„Çì„Å†‚Ä¶„ÅÇ„ÇåÊÄñ„Åè...        ja   \n",
       "\n",
       "                          created_at  \n",
       "0     Wed May 01 06:29:00 +0000 2019  \n",
       "1     Wed May 01 06:29:00 +0000 2019  \n",
       "2     Wed May 01 06:29:00 +0000 2019  \n",
       "3     Wed May 01 06:29:00 +0000 2019  \n",
       "4     Wed May 01 06:29:00 +0000 2019  \n",
       "...                              ...  \n",
       "6462  Wed May 01 06:31:45 +0000 2019  \n",
       "6463  Wed May 01 06:31:45 +0000 2019  \n",
       "6464  Wed May 01 06:31:45 +0000 2019  \n",
       "6465  Wed May 01 06:31:45 +0000 2019  \n",
       "6466  Wed May 01 06:31:45 +0000 2019  \n",
       "\n",
       "[6467 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user.lang</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@firawidya iya fir,hihi. Rencananya awal bulan...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 01:48:17 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>voo sair aki sem maldade, to com mt sono ; #fato</td>\n",
       "      <td>pt</td>\n",
       "      <td>Wed Sep 28 01:48:17 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RT @TheNoteboook: If he loves you, he'll care ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 01:48:17 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ombbb @ddlavato singing on abc!</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 01:48:17 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @aniserra: Nunca voy a entender como sabe e...</td>\n",
       "      <td>es</td>\n",
       "      <td>Wed Sep 28 01:48:15 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3691</td>\n",
       "      <td>@Agosbonelli a full!!! Despu√©s empiezan las ve...</td>\n",
       "      <td>es</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3692</td>\n",
       "      <td>@hay_ross ahaha, so am I, but we have our reas...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3693</td>\n",
       "      <td>@densukefukusima „Å§„Åæ„Çä„ÄÅ„Éó„É≠„É≠„Éº„Ç∞„ÇíÊ∂à„Åó„ÄÅÂõûÊÉ≥„Ç∑„Éº„É≥„ÇíÂÜíÈ†≠„Å´„Åô„Çã„Åì„Å®„Åß‰∏ÄÂ∑•...</td>\n",
       "      <td>ja</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3694</td>\n",
       "      <td>¬´@Socialite_Trina ‚ô™ Rolex/ More sex/good weed/...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3695</td>\n",
       "      <td>RT @AlbertoCiurana: La confianza se gana. La d...</td>\n",
       "      <td>en</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3696 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text user.lang  \\\n",
       "0     @firawidya iya fir,hihi. Rencananya awal bulan...        en   \n",
       "1      voo sair aki sem maldade, to com mt sono ; #fato        pt   \n",
       "2     RT @TheNoteboook: If he loves you, he'll care ...        en   \n",
       "3                       Ombbb @ddlavato singing on abc!        en   \n",
       "4     RT @aniserra: Nunca voy a entender como sabe e...        es   \n",
       "...                                                 ...       ...   \n",
       "3691  @Agosbonelli a full!!! Despu√©s empiezan las ve...        es   \n",
       "3692  @hay_ross ahaha, so am I, but we have our reas...        en   \n",
       "3693  @densukefukusima „Å§„Åæ„Çä„ÄÅ„Éó„É≠„É≠„Éº„Ç∞„ÇíÊ∂à„Åó„ÄÅÂõûÊÉ≥„Ç∑„Éº„É≥„ÇíÂÜíÈ†≠„Å´„Åô„Çã„Åì„Å®„Åß‰∏ÄÂ∑•...        ja   \n",
       "3694  ¬´@Socialite_Trina ‚ô™ Rolex/ More sex/good weed/...        en   \n",
       "3695  RT @AlbertoCiurana: La confianza se gana. La d...        en   \n",
       "\n",
       "                          created_at  \n",
       "0     Wed Sep 28 01:48:17 +0000 2011  \n",
       "1     Wed Sep 28 01:48:17 +0000 2011  \n",
       "2     Wed Sep 28 01:48:17 +0000 2011  \n",
       "3     Wed Sep 28 01:48:17 +0000 2011  \n",
       "4     Wed Sep 28 01:48:15 +0000 2011  \n",
       "...                              ...  \n",
       "3691  Wed Sep 28 02:38:59 +0000 2011  \n",
       "3692  Wed Sep 28 02:38:59 +0000 2011  \n",
       "3693  Wed Sep 28 02:38:59 +0000 2011  \n",
       "3694  Wed Sep 28 02:38:59 +0000 2011  \n",
       "3695  Wed Sep 28 02:38:59 +0000 2011  \n",
       "\n",
       "[3696 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2019\n",
    "tweets2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up The Data\n",
    "\n",
    "I have worked with a limited amount of my total data in order to make data processing go faster for my poor laptop. However, after testing the limits of my processor while I built the classifier, I want to add more to this notebook when I get the chance.\n",
    "\n",
    "As you can see, there are a lot of different languages in this dataset. The user.lang field in the dataframe does not describe the language of the tweet, but describes the users settings. This can cause us to keep data that isnt english and toss out actual english data. \n",
    "\n",
    "In order to clean that up i'm using the [langdetect](https://pypi.org/project/langdetect/) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am creating a dataframe to store the predicted languages in, cleaning up the text using regex, and then appending the language dataframe to the original tweet dataframe. I do this for both the 2011 and 2019 sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "lng = pd.DataFrame()\n",
    "while True: \n",
    "    try:\n",
    "        tweets2011.loc[i, \"text\"] = re.sub(r'(?:(@|&|;|http|https)[\\w_]+)', '', tweets2011.loc[i, \"text\"]) #cleaning\n",
    "        tmp = pd.DataFrame({'languages': [detect(tweets2011.loc[i, \"text\"])]}) \n",
    "        lng = lng.append(tmp, sort=False, ignore_index=True)\n",
    "        if i > len(tweets2011):\n",
    "            break\n",
    "        i = i+1\n",
    "    except:\n",
    "        tmp = pd.DataFrame({'languages': [\"error\"]}) #If the language is not able to be predicted\n",
    "        lng = lng.append(tmp, sort=False, ignore_index=True)\n",
    "        if i > len(tweets2011):\n",
    "            break\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as above, but for 2019\n",
    "i = 0\n",
    "lng = pd.DataFrame()\n",
    "while True: \n",
    "    try:\n",
    "        tweets2019.loc[i, \"text\"] = re.sub(r'(?:@[\\w_]+)', '', tweets2019.loc[i, \"text\"])\n",
    "        tmp = pd.DataFrame({'languages': [detect(tweets2019.loc[i, \"text\"])]})\n",
    "        lng = lng.append(tmp, sort=False, ignore_index=True)\n",
    "        if i > len(tweets2019):\n",
    "            break\n",
    "        i = i+1\n",
    "    except:\n",
    "        tmp = pd.DataFrame({'languages': [\"error\"]})\n",
    "        lng = lng.append(tmp, sort=False, ignore_index=True)\n",
    "        if i > len(tweets2011):\n",
    "            break\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2011['languages'] = lng\n",
    "tweets2011 = tweets2011.drop(columns=['user.lang'])\n",
    "tweets2019['languages'] = lng\n",
    "tweets2019 = tweets2019.drop(columns=['user.lang'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a breakdown of the top 5 languages represented during each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en       0.250812\n",
       "ja       0.213474\n",
       "error    0.149621\n",
       "th       0.066288\n",
       "ko       0.048431\n",
       "ar       0.047619\n",
       "es       0.042208\n",
       "id       0.021104\n",
       "pt       0.018939\n",
       "tl       0.014340\n",
       "tr       0.013528\n",
       "fr       0.010552\n",
       "ca       0.009470\n",
       "hi       0.008658\n",
       "de       0.008117\n",
       "it       0.006764\n",
       "so       0.006223\n",
       "vi       0.006223\n",
       "nl       0.006223\n",
       "ru       0.005682\n",
       "fa       0.003517\n",
       "sw       0.003517\n",
       "et       0.003517\n",
       "no       0.003247\n",
       "ur       0.003247\n",
       "af       0.002706\n",
       "cy       0.002706\n",
       "pl       0.002435\n",
       "ta       0.002165\n",
       "el       0.002165\n",
       "ro       0.002165\n",
       "fi       0.001623\n",
       "hu       0.001623\n",
       "sv       0.001623\n",
       "hr       0.001082\n",
       "da       0.001082\n",
       "zh-cn    0.000812\n",
       "sq       0.000812\n",
       "lt       0.000541\n",
       "ne       0.000541\n",
       "lv       0.000541\n",
       "bg       0.000541\n",
       "ml       0.000541\n",
       "zh-tw    0.000541\n",
       "mk       0.000541\n",
       "uk       0.000541\n",
       "sl       0.000271\n",
       "he       0.000271\n",
       "bn       0.000271\n",
       "te       0.000271\n",
       "sk       0.000271\n",
       "Name: languages, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "en       0.252153\n",
       "ja       0.213402\n",
       "error    0.149085\n",
       "th       0.065931\n",
       "ko       0.048439\n",
       "ar       0.047363\n",
       "es       0.042519\n",
       "id       0.021259\n",
       "pt       0.018837\n",
       "tl       0.014263\n",
       "tr       0.013455\n",
       "fr       0.010495\n",
       "ca       0.009419\n",
       "hi       0.008611\n",
       "de       0.008073\n",
       "it       0.006728\n",
       "so       0.006189\n",
       "vi       0.006189\n",
       "nl       0.006189\n",
       "ru       0.005651\n",
       "et       0.003498\n",
       "sw       0.003498\n",
       "fa       0.003498\n",
       "ur       0.003229\n",
       "no       0.003229\n",
       "af       0.002691\n",
       "cy       0.002691\n",
       "pl       0.002422\n",
       "ro       0.002422\n",
       "ta       0.002153\n",
       "el       0.002153\n",
       "hu       0.001615\n",
       "fi       0.001615\n",
       "sv       0.001615\n",
       "da       0.001076\n",
       "hr       0.001076\n",
       "sq       0.000807\n",
       "zh-cn    0.000807\n",
       "lt       0.000538\n",
       "mk       0.000538\n",
       "uk       0.000538\n",
       "lv       0.000538\n",
       "ml       0.000538\n",
       "zh-tw    0.000538\n",
       "bg       0.000538\n",
       "ne       0.000538\n",
       "sk       0.000269\n",
       "te       0.000269\n",
       "sl       0.000269\n",
       "he       0.000269\n",
       "bn       0.000269\n",
       "Name: languages, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2011['languages'].value_counts(5)\n",
    "tweets2019['languages'].value_counts(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am paring down the dataframe to only have english as the language. It is not 100% accurate, but it judges the tweet rather than the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Made a Wendy's pit stop</td>\n",
       "      <td>Wed Sep 28 01:48:18 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>No sappy love songs tn... just please.</td>\n",
       "      <td>Wed Sep 28 01:48:18 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>knp siyy hrs nge ping berkali2 (‚å£ÃÅ_‚å£ÃÄ)</td>\n",
       "      <td>Wed Sep 28 01:48:18 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>\"eu te amo\" verdadeiros sempre s√£o bem-vindos</td>\n",
       "      <td>Wed Sep 28 01:48:18 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>#JevaVacana‚ô•</td>\n",
       "      <td>Wed Sep 28 01:48:18 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>3665</td>\n",
       "      <td>haha thats weird af. Cuz i said haly would be...</td>\n",
       "      <td>Wed Sep 28 02:38:58 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>3667</td>\n",
       "      <td>saludos afectuosos!!!!</td>\n",
       "      <td>Wed Sep 28 02:38:58 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>3668</td>\n",
       "      <td>RT : My favorite kinds of movies: Disney, Pixa...</td>\n",
       "      <td>Wed Sep 28 02:38:58 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3670</td>\n",
       "      <td>Beeeeeeija aeew , queremos show\\n ( live on ht...</td>\n",
       "      <td>Wed Sep 28 02:38:58 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>3684</td>\n",
       "      <td>Can't wait till I get rid of this cough</td>\n",
       "      <td>Wed Sep 28 02:38:59 +0000 2011</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>927 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "0        8                            Made a Wendy's pit stop   \n",
       "1       12             No sappy love songs tn... just please.   \n",
       "2       18             knp siyy hrs nge ping berkali2 (‚å£ÃÅ_‚å£ÃÄ)   \n",
       "3       19      \"eu te amo\" verdadeiros sempre s√£o bem-vindos   \n",
       "4       21                                       #JevaVacana‚ô•   \n",
       "..     ...                                                ...   \n",
       "922   3665   haha thats weird af. Cuz i said haly would be...   \n",
       "923   3667                             saludos afectuosos!!!!   \n",
       "924   3668  RT : My favorite kinds of movies: Disney, Pixa...   \n",
       "925   3670  Beeeeeeija aeew , queremos show\\n ( live on ht...   \n",
       "926   3684            Can't wait till I get rid of this cough   \n",
       "\n",
       "                         created_at languages  \n",
       "0    Wed Sep 28 01:48:18 +0000 2011        en  \n",
       "1    Wed Sep 28 01:48:18 +0000 2011        en  \n",
       "2    Wed Sep 28 01:48:18 +0000 2011        en  \n",
       "3    Wed Sep 28 01:48:18 +0000 2011        en  \n",
       "4    Wed Sep 28 01:48:18 +0000 2011        en  \n",
       "..                              ...       ...  \n",
       "922  Wed Sep 28 02:38:58 +0000 2011        en  \n",
       "923  Wed Sep 28 02:38:58 +0000 2011        en  \n",
       "924  Wed Sep 28 02:38:58 +0000 2011        en  \n",
       "925  Wed Sep 28 02:38:58 +0000 2011        en  \n",
       "926  Wed Sep 28 02:38:59 +0000 2011        en  \n",
       "\n",
       "[927 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2011 = tweets2011[tweets2011['languages'] == 'en']\n",
    "len(tweets2011[tweets2011['languages'] == 'en'])\n",
    "tweets2011.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>RT : Rep. Ted Lieu, member of the House Judici...</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>RT : https://t.co/oTwwTOmEzD</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>RT : Happy birthday to the man of steel.. Our ...</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>I give up on these guys</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>lmaooo the accuracy</td>\n",
       "      <td>Wed May 01 06:29:00 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>3703</td>\n",
       "      <td>For Black Ice Booking, Features and Booking St...</td>\n",
       "      <td>Wed May 01 06:30:33 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>3705</td>\n",
       "      <td>RT : https://t.co/gunEkOlQoQ</td>\n",
       "      <td>Wed May 01 06:30:33 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>3708</td>\n",
       "      <td>Wish me luck „Ö†„Ö†„Ö†</td>\n",
       "      <td>Wed May 01 06:30:33 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>3710</td>\n",
       "      <td>Come to Dubai then üòã</td>\n",
       "      <td>Wed May 01 06:30:33 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>3711</td>\n",
       "      <td>RT : Cute couple \\n\\nBahutahi cute couple\\n\\n ...</td>\n",
       "      <td>Wed May 01 06:30:33 +0000 2019</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "0        8  RT : Rep. Ted Lieu, member of the House Judici...   \n",
       "1       12                       RT : https://t.co/oTwwTOmEzD   \n",
       "2       18  RT : Happy birthday to the man of steel.. Our ...   \n",
       "3       19                            I give up on these guys   \n",
       "4       21                                lmaooo the accuracy   \n",
       "..     ...                                                ...   \n",
       "932   3703  For Black Ice Booking, Features and Booking St...   \n",
       "933   3705                       RT : https://t.co/gunEkOlQoQ   \n",
       "934   3708                                   Wish me luck „Ö†„Ö†„Ö†   \n",
       "935   3710                               Come to Dubai then üòã   \n",
       "936   3711  RT : Cute couple \\n\\nBahutahi cute couple\\n\\n ...   \n",
       "\n",
       "                         created_at languages  \n",
       "0    Wed May 01 06:29:00 +0000 2019        en  \n",
       "1    Wed May 01 06:29:00 +0000 2019        en  \n",
       "2    Wed May 01 06:29:00 +0000 2019        en  \n",
       "3    Wed May 01 06:29:00 +0000 2019        en  \n",
       "4    Wed May 01 06:29:00 +0000 2019        en  \n",
       "..                              ...       ...  \n",
       "932  Wed May 01 06:30:33 +0000 2019        en  \n",
       "933  Wed May 01 06:30:33 +0000 2019        en  \n",
       "934  Wed May 01 06:30:33 +0000 2019        en  \n",
       "935  Wed May 01 06:30:33 +0000 2019        en  \n",
       "936  Wed May 01 06:30:33 +0000 2019        en  \n",
       "\n",
       "[937 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2019 = tweets2019[tweets2019['languages'] == 'en']\n",
    "len(tweets2019[tweets2019['languages'] == 'en'])\n",
    "tweets2019.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing The Data\n",
    "\n",
    "Now we are going to analyze the tweets in the first of two ways: Frequency Distrubution\n",
    "\n",
    "In order to do that i'm going to tokenize all the tweets for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks2011 = [word_tokenize(i) for i in tweets2011['text']]\n",
    "toks2019 = [word_tokenize(i) for i in tweets2019['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Made', 'a', 'Wendy', \"'s\", 'pit', 'stop'], ['No', 'sappy', 'love', 'songs', 'tn', '...', 'just', 'please', '.'], ['knp', 'siyy', 'hrs', 'nge', 'ping', 'berkali2', '(', '‚å£ÃÅ_‚å£ÃÄ', ')'], ['``', 'eu', 'te', 'amo', \"''\", 'verdadeiros', 'sempre', 's√£o', 'bem-vindos'], ['#', 'JevaVacana‚ô•']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['RT', ':', 'Rep.', 'Ted', 'Lieu', ',', 'member', 'of', 'the', 'House', 'Judiciary', 'Committee', ',', 'tells', 'CNN', \"'s\", 'that', 'if', 'Attorney', 'General', 'Bill', 'Barr', 'does', 'not', 'a‚Ä¶'], ['RT', ':', 'https', ':', '//t.co/oTwwTOmEzD'], ['RT', ':', 'Happy', 'birthday', 'to', 'the', 'man', 'of', 'steel..', 'Our', '#', 'ThalaAjith', 'Sir', '...', 'May', 'u', 'haveee', 'a', 'lonnnngggggggg', 'lonnngggggggg', 'life', 'filled', 'with', 'loa‚Ä¶'], ['I', 'give', 'up', 'on', 'these', 'guys'], ['lmaooo', 'the', 'accuracy']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks2011[:5]\n",
    "toks2019[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toks2011` and `toks2019` are in a nested list format, and in order to check the frequency distrubution of all text in either year we need to flatten them into one list. The following code does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_2011_toks = []\n",
    "flat_2019_toks = []\n",
    "def remove2011Nesting(l): \n",
    "    for i in l: \n",
    "        if type(i) == list: \n",
    "            remove2011Nesting(i) \n",
    "        else: \n",
    "            flat_2011_toks.append(i) \n",
    "            \n",
    "def remove2019Nesting(l): \n",
    "    for i in l: \n",
    "        if type(i) == list: \n",
    "            remove2019Nesting(i) \n",
    "        else: \n",
    "            flat_2019_toks.append(i)\n",
    "  \n",
    "remove2011Nesting(toks2011) \n",
    "remove2019Nesting(toks2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am removing all punctuation, blank elements, and twitter specific lingo like \"RT\" for retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "flat_2011_toks = [w.translate(table) for w in flat_2011_toks]\n",
    "flat_2019_toks = [w.translate(table) for w in flat_2019_toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noRT = str.maketrans('', '', 'RT')\n",
    "flat_2011_toks = [w.translate(noRT) for w in flat_2011_toks]\n",
    "flat_2019_toks = [w.translate(noRT) for w in flat_2019_toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Made', 'a', 'Wendy', 's', 'pit', 'stop', 'No', 'sappy', 'love', 'songs']\n",
      "['ep', 'ed', 'Lieu', 'member', 'of', 'the', 'House', 'Judiciary', 'Committee', 'tells']\n"
     ]
    }
   ],
   "source": [
    "while(\"\" in flat_2011_toks): \n",
    "    flat_2011_toks.remove(\"\") \n",
    "    \n",
    "while(\"\" in flat_2019_toks): \n",
    "    flat_2019_toks.remove(\"\")\n",
    "    \n",
    "print(flat_2011_toks[:10])   \n",
    "print(flat_2019_toks[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we can take the frequency distrubution of both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'I': 141, 'a': 136, 'to': 97, 'http': 96, 'the': 87, 'que': 72, 'de': 70, 'me': 69, 'you': 62, 'of': 55, ...})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'https': 463, 'the': 299, 'to': 266, 'a': 206, 'and': 184, '‚Äô': 171, 'I': 163, 'you': 158, 'of': 153, 'is': 127, ...})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd2011 = nltk.FreqDist(flat_2011_toks)\n",
    "fd2011\n",
    "\n",
    "fd2019 = nltk.FreqDist(flat_2019_toks)\n",
    "fd2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most common words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https', 463), ('the', 299), ('to', 266), ('a', 206), ('and', 184), ('‚Äô', 171), ('I', 163), ('you', 158), ('of', 153), ('is', 127), ('s', 124), ('for', 109), ('in', 98), ('on', 91), ('this', 88), ('he', 86), ('it', 83), ('my', 81), ('‚†Ä', 80), ('that', 76), ('me', 67), ('with', 61), ('BBMAsopSocial', 58), ('BS', 56), ('his', 55), ('was', 54), ('at', 52), ('be', 50), ('‚Ä¶', 49), ('i', 48), ('1', 44), ('are', 41), ('so', 41), ('not', 40), ('t', 39), ('if', 38), ('amp', 38), ('we', 38), ('do', 38), ('your', 37), ('from', 36), ('nt', 35), ('all', 35), ('can', 35), ('see', 34), ('m', 34), ('like', 33), ('have', 33), ('how', 32), ('up', 31), ('but', 31), ('when', 29), ('who', 29), ('out', 28), ('‚Äú', 28), ('re', 28), ('will', 28), ('one', 28), ('just', 27), ('her', 25), ('‚Äù', 24), ('they', 24), ('people', 24), ('as', 23), ('than', 22), ('My', 22), ('by', 22), ('what', 21), ('want', 21), ('here', 20)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('I', 141), ('a', 136), ('to', 97), ('http', 96), ('the', 87), ('que', 72), ('de', 70), ('me', 69), ('you', 62), ('of', 55), ('and', 54), ('it', 53), ('my', 50), ('no', 48), ('y', 41), ('is', 40), ('that', 38), ('s', 37), ('on', 37), ('o', 34), ('in', 34), ('nt', 34), ('like', 33), ('el', 32), ('do', 32), ('m', 29), ('te', 28), ('i', 28), ('la', 27), ('e', 27), ('be', 26), ('for', 26), ('he', 26), ('just', 25), ('so', 25), ('not', 24), ('this', 24), ('have', 24), ('with', 22), ('por', 22), ('lol', 22), ('at', 21), ('√©', 21), ('se', 20), ('your', 20), ('en', 20), ('what', 20), ('u', 19), ('es', 19), ('eu', 18), ('when', 18), ('get', 18), ('tuts', 18), ('but', 17), ('got', 17), ('if', 17), ('A', 17), ('da', 17), ('out', 16), ('can', 16), ('are', 16), ('q', 16), ('un', 16), ('as', 15), ('they', 15), ('all', 15), ('time', 15), ('shit', 14), ('na', 14), ('ll', 14)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd2019.most_common(70)\n",
    "fd2011.most_common(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there are still some foreign words that slipped through langdetect, something to work on for the future. \n",
    "Just taking a look at the most common for the two years I can see that most of the non-english words seem to be concentrated in 2011 and kpop is more talked about in 2019 (\"BBMAs\", \"BTS\" being indicators). These are interesting to note, but don't get at the question I want to be asking with my sentiment analysis.\n",
    "\n",
    "So I compared frequencies of certain words occurring in 2011 versus 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of...\n",
      "\n",
      "The word \"u\" in 2011: 19 versus 2019: 14\n",
      "The word \"lol\" in 2011: 22 versus 2019: 3\n",
      "\n",
      "\n",
      "The word \"excited\" in 2011: 2 versus 2019: 1\n",
      "The word \"happy\" in 2011: 3 versus 2019: 2\n",
      "The word \"depressed\" in 2011: 0 versus 2019: 1\n",
      "The word \"hope\" in 2011: 2 versus 2019: 3\n",
      "The word \"love\" in 2011: 11 versus 2019: 20\n",
      "The word \"hate\" in 2011: 5 versus 2019: 3\n",
      "\n",
      "\n",
      "The word \"they\" in 2011: 15 versus 2019: 24\n",
      "The word \"transgender\" in 2011: 0 versus 2019: 1\n",
      "The word \"election\" in 2011: 0 versus 2019: 1\n",
      "The word \"race\" in 2011: 2 versus 2019: 0\n",
      "The word \"gay\" in 2011: 1 versus 2019: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequency of...\\n\")\n",
    "print(\"The word \\\"u\\\" in 2011:\",fd2011[\"u\"], \"versus 2019:\",fd2019[\"u\"])\n",
    "print(\"The word \\\"lol\\\" in 2011:\",fd2011[\"lol\"], \"versus 2019:\",fd2019[\"lol\"])\n",
    "print('\\n')\n",
    "print(\"The word \\\"excited\\\" in 2011:\",fd2011[\"excited\"], \"versus 2019:\",fd2019[\"excited\"])\n",
    "print(\"The word \\\"happy\\\" in 2011:\",fd2011[\"happy\"], \"versus 2019:\",fd2019[\"happy\"])\n",
    "print(\"The word \\\"depressed\\\" in 2011:\",fd2011[\"depressed\"], \"versus 2019:\",fd2019[\"depressed\"])\n",
    "print(\"The word \\\"hope\\\" in 2011:\",fd2011[\"hope\"], \"versus 2019:\",fd2019[\"hope\"])\n",
    "print(\"The word \\\"love\\\" in 2011:\",fd2011[\"love\"], \"versus 2019:\",fd2019[\"love\"])\n",
    "print(\"The word \\\"hate\\\" in 2011:\",fd2011[\"hate\"], \"versus 2019:\",fd2019[\"hate\"])\n",
    "print('\\n')\n",
    "print(\"The word \\\"they\\\" in 2011:\",fd2011[\"they\"], \"versus 2019:\",fd2019[\"they\"])\n",
    "print(\"The word \\\"transgender\\\" in 2011:\",fd2011[\"transgender\"], \"versus 2019:\",fd2019[\"transgender\"])\n",
    "print(\"The word \\\"election\\\" in 2011:\",fd2011[\"election\"], \"versus 2019:\",fd2019[\"election\"])\n",
    "print(\"The word \\\"race\\\" in 2011:\",fd2011[\"race\"], \"versus 2019:\",fd2019[\"race\"])\n",
    "print(\"The word \\\"gay\\\" in 2011:\",fd2011[\"gay\"], \"versus 2019:\",fd2019[\"gay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I divided them up into a few groups:\n",
    "- Texting lingo\n",
    "- \"Feeling\" words\n",
    "- Current topics of conversation in pop culture\n",
    "\n",
    "For texting lingo, I was surprised at how completely \"lol\" has gone out of style compared to 2011. From my own observations, \"lol\" is used more sarcasticly and has a negative connotation these days, but I was definitely expecting to see it. \"u\" as a substitue for \"you\" still seems to be standing strong against the change of time. \n",
    "\n",
    "With \"feeling\" words I targeted words that are used to describe sentiment towards an object or person. The data was too sparse to make any notable connections, other than for the word \"love\" where 2019 triumphs. My guess for why this could be is that there is a cultural \"self care\" renaissance happening online where people are taking the time to think about mental health. Maybe this increase in love comes from that trend?\n",
    "\n",
    "Finally, the current events section is where I targetted words that are \"buzz words\" in this day and age and are talked about on political twitter. The one that surprised me was the increase in the pronoun \"they\". In the past 10 years there has been a spotlight on transgender issues, namely using \"they\" as a gender neutral pronoun. My guess is that conversation promoted the increase in use of that pronoun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this look into the frequecy of words leaves a lot to be desired. I also wanted to create bigrams to take a closer look at the context of some of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2011 = list(nltk.ngrams(flat_2011_toks, 2))\n",
    "bigram2019 = list(nltk.ngrams(flat_2019_toks, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2011fd = nltk.FreqDist(bigram2011)\n",
    "bigram2019fd = nltk.FreqDist(bigram2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('I', 'm'), 27), (('tuts', 'tuts'), 17), (('do', 'nt'), 12), (('of', 'the'), 9), (('to', 'be'), 8), (('a', 'la'), 7), (('I', 'll'), 7), (('I', 'do'), 7), (('on', 'the'), 7), (('of', 'my'), 6), (('I', 'can'), 6), (('in', 'the'), 6), (('it', 's'), 6), (('wan', 'na'), 5), (('this', 'is'), 5), (('for', 'the'), 5), (('going', 'to'), 5), (('need', 'to'), 5), (('I', 'want'), 5), (('want', 'to'), 5), (('I', 'got'), 4), (('if', 'you'), 4), (('that', 's'), 4), (('you', 're'), 4), (('in', 'my'), 4), (('got', 'ta'), 4), (('Do', 'nt'), 4), (('to', 'do'), 4), (('have', 'a'), 4), (('when', 'I'), 4), (('I', 'am'), 4), (('si', 'no'), 4), (('que', 'no'), 4), (('o', 'que'), 4), (('is', 'a'), 4), (('que', '√©'), 4), (('en', 'la'), 4), (('te', 'amo'), 3), (('out', 'and'), 3), (('it', 'is'), 3), (('but', 'I'), 3), (('how', 'to'), 3), (('needs', 'to'), 3), (('de', 'tudo'), 3), (('que', 'tem'), 3), (('at', 'the'), 3), (('not', 'be'), 3), (('my', 'life'), 3), (('to', 'a'), 3), (('to', 'me'), 3)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(('‚†Ä', '‚†Ä'), 72), (('‚Äô', 's'), 63), (('‚Äô', 't'), 38), (('BBMAsopSocial', 'BS'), 31), (('of', 'the'), 26), (('I', '‚Äô'), 25), (('in', 'the'), 22), (('BS', 'https'), 18), (('‚Äô', 'm'), 18), (('BBMAsopSocial', 'EXO'), 17), (('is', 'a'), 17), (('if', 'you'), 16), (('‚Äô', 're'), 15), (('I', 'm'), 15), (('his', 'is'), 14), (('to', 'be'), 14), (('do', 'nt'), 14), (('If', 'you'), 13), (('to', 'see'), 12), (('for', 'the'), 12), (('to', 'the'), 11), (('on', 'the'), 11), (('you', '‚Äô'), 11), (('with', 'the'), 11), (('1', 'Vote'), 10), (('is', 'the'), 9), (('from', 'the'), 9), (('in', 'a'), 9), (('don', '‚Äô'), 9), (('it', 'to'), 8), (('all', 'the'), 8), (('the', 'best'), 8), (('want', 'to'), 8), (('it', '‚Äô'), 8), (('‚Ä¶', 'https'), 8), (('and', 'it'), 8), (('EXO', 'https'), 8), (('but', 'I'), 8), (('this', 'is'), 8), (('Happy', 'birthday'), 7), (('you', 'are'), 7), (('going', 'to'), 7), (('to', 'get'), 7), (('to', 'vote'), 7), (('on', 'my'), 7), (('have', 'a'), 7), (('it', 'was'), 7), (('BBMAsAchievement', 'Lady'), 7), (('Lady', 'Gaga'), 7), (('Gaga', 'amp'), 7)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2011fd.most_common(50)\n",
    "bigram2019fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of...\n",
      "\n",
      "The word \"don't\" in 2011: 12 versus 2019: 14\n",
      "The word \"can't\" in 2011: 2 versus 2019: 4\n",
      "The word \"won't\" in 2011: 3 versus 2019: 0\n",
      "The word \"aren't\" in 2011: 1 versus 2019: 1\n",
      "The word \"ain't\" in 2011: 2 versus 2019: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequency of...\\n\")\n",
    "print(\"The word \\\"don't\\\" in 2011:\",bigram2011fd[('do', 'nt')], \"versus 2019:\",bigram2019fd[('do', 'nt')])\n",
    "print(\"The word \\\"can't\\\" in 2011:\",bigram2011fd[('ca', 'nt')], \"versus 2019:\",bigram2019fd[('ca', 'nt')])\n",
    "print(\"The word \\\"won't\\\" in 2011:\",bigram2011fd[('wo', 'nt')], \"versus 2019:\",bigram2019fd[('wo', 'nt')])\n",
    "print(\"The word \\\"aren't\\\" in 2011:\",bigram2011fd[('are', 'nt')], \"versus 2019:\",bigram2019fd[('are', 'nt')])\n",
    "print(\"The word \\\"ain't\\\" in 2011:\",bigram2011fd[('ai', 'nt')], \"versus 2019:\",bigram2019fd[('ai', 'nt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at the frequency of negations between the two years. For the most part, there are no glaring differences between the two eras. Let's look at more substansial tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words \"love you\" in 2011: 1 versus 2019: 4\n",
      "The words \"love you\" in 2011: 0 versus 2019: 0\n",
      "The word \"hate it\" in 2011: 1 versus 2019: 0\n",
      "\n",
      "\n",
      "The word \"I can't\" in 2011: 0 versus 2019: 1\n",
      "The word \"voting for\" in 2011: 0 versus 2019: 4\n",
      "\n",
      "\n",
      "The word \"can do\" in 2011: 1 versus 2019: 4\n",
      "The word \"can do\" in 2011: 0 versus 2019: 0\n",
      "The word \"ca/wo/do n't do\" in 2011: 0 versus 2019: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The words \\\"love you\\\" in 2011:\",bigram2011fd[('love', 'you')], \"versus 2019:\",bigram2019fd[('love', 'you')])\n",
    "print(\"The words \\\"love you\\\" in 2011:\",bigram2011fd[('love', 'myself')], \"versus 2019:\",bigram2019fd[('love', 'myself')])\n",
    "print(\"The word \\\"hate it\\\" in 2011:\",bigram2011fd[('hate', 'it')], \"versus 2019:\",bigram2019fd[('hate', 'it')])\n",
    "print('\\n')\n",
    "print(\"The word \\\"I can't\\\" in 2011:\",bigram2011fd[('I', 'ca')], \"versus 2019:\",bigram2019fd[('I', 'ca')])\n",
    "print(\"The word \\\"voting for\\\" in 2011:\",bigram2011fd[('voting', 'for')], \"versus 2019:\",bigram2019fd[('voting', 'for')])\n",
    "print('\\n')\n",
    "print(\"The word \\\"can do\\\" in 2011:\",bigram2011fd[('can', 'do')], \"versus 2019:\",bigram2019fd[('can', 'do')])\n",
    "print(\"The word \\\"can do\\\" in 2011:\",bigram2011fd[('nothing', 'i')], \"versus 2019:\",bigram2019fd[('nothing', 'i')])\n",
    "print(\"The word \\\"ca/wo/do n't do\\\" in 2011:\",bigram2011fd[('nt', 'do')], \"versus 2019:\",bigram2019fd[('nt', 'do')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I looked at tuples, building off of some of the single terms I looked at earlier. The use of \"love you\" slightly increased since 2011, however there are no instances of \"love myself\" which disproves my hypothesis about the \"self care\" trend.\n",
    "\n",
    "Interestingly the usage of \"can do\" also increased, implying that tweets have become more hopeful (\"nothing i\" shows that the phrase isn't \"nothing i can do\" or something similar). \n",
    "\n",
    "Additionally, the bigram \"voting for\" going from 0 to 4 reflects the time period for these tweets well. \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Now finally the data we unpickled for the classifier is going to come into play here. I am redefining the method to create document_features so that it can be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of ['RT', ':', 'Live', 'everyday', 'as', 'if', 'we', \"'re\", 'waking', 'up', 'for', 'the', 'first', 'time', '.', 'Appreciate', 'life', 'as', 'if', 'it', \"'s\", 'the', 'last', 'time', 'we', \"'re\", 'going', 'to', 'sleep', '.'] \n",
      "is 4\n",
      "Classification of ['RT', ':', 'Thanks', 'for', 'coming', '!', '!', 'Glad', 'to', 'hear', 'you', 'enjoyed', 'it', '.'] \n",
      "is 4\n",
      "Classification of ['I', 'never', 'talked', 'to', 'a', 'female', 'outside', 'my', 'race', 'so', 'this', 'is', 'very', 'new', 'to', 'me', 'o_O'] \n",
      "is 0\n",
      "Classification of ['...', 'no', '.', 'But', 'add', 'ambition', 'to', 'that', 'and', 'experience', '=', 'unstoppable', ',', 'happy', ',', 'success', '!'] \n",
      "is 4\n",
      "Classification of ['Gahh.i', 'want', 'both', '...', 'yoontae..but', 'now', 'i', 'want', 'Yul', 'pic', '!', '!', '!'] \n",
      "is 0\n",
      "Classification of ['I', 'swear', 'I', 'treat', 'my', 'car', 'like', 'a', 'sports', 'car', '!'] \n",
      "is 4\n",
      "Classification of ['i', 'got', 'ta', 'be', 'a', 'mid-20', \"'s\", 'woman..errbody', 'wonderin', 'y', 'ion', 'have', 'kids', 'yet', '...', '.well', 'it', 'will', 'happen', 'one', 'day', 'folks', '...', 'trust', 'me'] \n",
      "is 0\n",
      "Classification of ['Giroux', '.', 'He', 'will', 'show', 'all', 'his', 'talent', '.'] \n",
      "is 4\n"
     ]
    }
   ],
   "source": [
    "twt1 = document_features(toks2011[192])\n",
    "twt2 = document_features(toks2011[76]) \n",
    "twt3 = document_features(toks2011[23]) \n",
    "twt4 = document_features(toks2011[892])\n",
    "twt5 = document_features(toks2011[677])\n",
    "twt6 = document_features(toks2011[655]) \n",
    "twt7 = document_features(toks2011[793]) \n",
    "twt8 = document_features(toks2011[93])\n",
    "print(\"Classification of\",toks2011[192],\"\\nis\",classifier.classify(twt1))\n",
    "print(\"Classification of\", toks2011[76],\"\\nis\",classifier.classify(twt2))\n",
    "print(\"Classification of\", toks2011[23],\"\\nis\",classifier.classify(twt3))\n",
    "print(\"Classification of\", toks2011[892],\"\\nis\",classifier.classify(twt4))\n",
    "print(\"Classification of\",toks2011[677],\"\\nis\",classifier.classify(twt5))\n",
    "print(\"Classification of\", toks2011[655],\"\\nis\",classifier.classify(twt6))\n",
    "print(\"Classification of\", toks2011[793],\"\\nis\",classifier.classify(twt7))\n",
    "print(\"Classification of\", toks2011[93],\"\\nis\",classifier.classify(twt8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick look at the classifier, it seems to be getting the tweets somewhat correct. My next step will be to improve the classifier's accuracy before I perform an analysis on the tweets from 2011 and 2019 to see which of them are more positive or negative respectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
